{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-â€‹001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 1\n",
    "\n",
    "\n",
    "### Name: Bozhou Yan\n",
    "\n",
    "\n",
    "### Due: Oct. 2, 2019\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing a logistic regression model and gradient descent as well as to explore some theoretical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent (40 pts total)\n",
    "\n",
    "In this problem we will study gradient descent for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (10pts) Implementing gradient descent with a fixed learning rate\n",
    "\n",
    "Using autograd, implement gradient descent with a fixed learning rate for a general function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    fun : function\n",
    "    x0  : initial point\n",
    "    lr  : fixed learning rate\n",
    "    iterations : number of iterations to perform\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun\n",
    "\"\"\"\n",
    "def gradient_descent_fixed(fun, x0, lr, iterations):\n",
    "   \n",
    "    x=np.asarray(x0).astype('float64')\n",
    "    for i in range(0,iterations):\n",
    "        grad_fun=grad(fun)\n",
    "        #print(\"iteration=\",i,\"current x \",x,\"current grad: \",grad_fun(x))\n",
    "        x=x-(lr*grad_fun(x))\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Using a variable learning rate\n",
    "\n",
    "Sometimes it is necessary to decrease our learning rate as we iterate to help gradient descent converge.  Implement gradient descent below where the learning rate at iteration $i$ is given by\n",
    "$$\n",
    "\\mathrm{lr}_i = \\frac{\\mathrm{lr}_0}{i+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    fun : function\n",
    "    x0  : initial point\n",
    "    lr  : initial learning rate\n",
    "    iterations : number of iterations to perform\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun\n",
    "\"\"\"\n",
    "def gradient_descent_variable(fun, x0, lr, iterations): \n",
    "    grad_fun=grad(fun)\n",
    "    x=np.asarray(x0).astype('float64')\n",
    "    lr_variable=lr\n",
    "    for i in range(0,iterations):\n",
    "        lr_variable=lr/(i+1)\n",
    "        x=x-lr_variable*grad_fun(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts)  Choosing the learning rate\n",
    "\n",
    "Let $\\alpha$ denote the learning rate and consider the function $f(x) = \\frac{1}{2}x^2$.  The gradient descent update rule at iteration $n+1$ is given by\n",
    "$$\n",
    "x_{n+1} = x_n - \\alpha f'(x_n)\n",
    "$$\n",
    "Is there a critical value $\\alpha_0$ so that if $\\alpha \\ge \\alpha_0$ then gradient descent will not converge for $f(x)$?  If so what is it and explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer goes here: Yes, $ \\alpha_0 =2 $ would be such a critical value. Since $ f'(x)=x $ we know $x_{n+1}= (1-\\alpha)x_n $, if alpha is greater than 2 then $|x|$ will increase and never converge; if $\\alpha = 2$ then this update rule will only flip the sign of x and do nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)  (10 pts) Feature scaling\n",
    "\n",
    "Oftentimes it is advantageous to rescale or normalize our features.  As a toy example suppose we want to predict a person's weight (in kgs.) based on their height.  We would like an algorithm that gives equally good predictions whether height is measured in centimeters or kilometers.  For a concrete example, consider the 2D optimization problems:\n",
    "$$\n",
    "\\mathrm{argmin}_{x\\in \\mathbb{R}^2}\\ x^T \\Sigma_i x, \\quad i = 1,2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}, \\quad \n",
    "\\Sigma_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 100\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Suppose our starting point is the same $x_0 = (1, 1)^T$ for both optimization problems.  For a fixed learning rate, will gradient descent perform better on problem $i=1$ or $i=2$?  Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer goes here: It will likely perform better on problem i=1 since in i=2 the second field of x will have a significantly higher impact on the gradient than the first field, thus resulting in the algorithm facing more trouble while tring to locate the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Logistic Regression (60 pts total)\n",
    "\n",
    "In this problem you will implement all of the steps that are taken care of whenever the \"fit\" function from sci-kit is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (15 pts) Logisitc Unit\n",
    "\n",
    "For binary classification $y \\in \\{0,1\\}$ we will model the posterior probability $p(y = 1 | x)$ with the logistic unit \n",
    "$$\n",
    "h(x; w) = \\frac{1}{1 + \\exp(-w^Tx)}\n",
    "$$\n",
    "We will use the convention that $x_0 = 1$.  Implement this function below using the skeletal outline as a guide.  Suppose we use a discriminant function $f(x)$ which assigns the label $y = 1$ if $p(y = 1 | x) \\ge 0.5$ and $0$ otherwise.  In other words, with a discriminant function we do not need the posterior probabilities but rather skip straight to the classification.  \n",
    "\n",
    "\n",
    "Give a geometric interpretation of our classifier once the parameters $w$ have been learned.  What does the angle between $w$ and $x$ tell us about the predicted class value?  For which angles do we predict $y=1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    w: weight vector\n",
    "    X: data\n",
    "\n",
    "Return:\n",
    "    logits : h(x;w)\n",
    "\"\"\"\n",
    "def logistic_unit(X, w):\n",
    "    logits=np.reciprocal(1+np.exp(-np.dot(X,w)))\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer goes here: If the angle between W and x is large, then $ -w^T x$ becomes large as well, then the denominator of our logit also gets large which makes the probability small, and vice versa. For angles less than 90 degrees, $ exp(- w^T x) <1 $ therefore $h(x;w)>0.5$ and we predict y=1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Deriving the loss function\n",
    "\n",
    "We have implicitly made the assumption that $y|x \\sim \\mathrm{Bernoulli}(h(x;w))$.  If we have an iid dataset $\\{(x_i,y_i)\\}_{i=1}^N$, then we can write the data likelihood as\n",
    "$$\n",
    "p(\\vec{y}| X, w) = \\prod_{i=1}^N p(y_i | x_i, w)\n",
    "$$\n",
    "We can learn the parameter $w$ by maximizing this probability (i.e. we find the MLE) or equivalently minimizing $J(w) = -\\log p(\\vec{y} | X, w)$.  Derive the loss function $J(w)$ step-by-step and implement it using the skeletal outline below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation goes here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    w : weight vector\n",
    "    X : dataset features\n",
    "    y : dataset targets\n",
    "\n",
    "Return:\n",
    "    J : loss of w given X,y\n",
    "\"\"\"\n",
    "def loss(w, X, y):\n",
    "    \n",
    "    cumulated_log_likelihood=0\n",
    "    \n",
    "    for i in range(0,len(X)):\n",
    "        current_logit=logistic_unit(X[i],w)\n",
    "      \n",
    "        #print(cumulated_likelihood,\"**\",current_logit)\n",
    "        \n",
    "        cumulated_log_likelihood=cumulated_log_likelihood-np.log(np.power(current_logit,y[i])*np.power((1-current_logit),(1-y[i])))\n",
    "    \n",
    "    #print(\"cl\",cumulated_likelihood)      \n",
    "    J=cumulated_log_likelihood\n",
    "    \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (5 pts) Splitting the data for training and testing\n",
    "\n",
    "Now we'll actually learn a logistic regression model for some synthetic data.\n",
    "\n",
    "First split the dataset into a training, validation, and test set.  Use a 40/40/20 split (roughly 40/40/20 is fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate some fake data.\n",
    "X, y = make_blobs(n_samples = 10000, centers = 2, n_features =20, cluster_std =20)\n",
    "\n",
    "# We'll also augment the data so that x_0 = 1 for the intercept term.\n",
    "X = np.append(np.ones((len(X), 1)), X, axis = 1)\n",
    "\n",
    "index=[]\n",
    "\n",
    "for i in range(0,10000):\n",
    "    index.append(i)\n",
    "    \n",
    "np.random.shuffle(index)\n",
    "\n",
    "\"\"\"Randomize the data cuz why not\"\"\"\n",
    "\n",
    "shuffled_X=[]\n",
    "shuffled_y=[]\n",
    "\n",
    "for i in range(0,10000):\n",
    "    shuffled_X.append(X[index[i]])\n",
    "    shuffled_y.append(y[index[i]])\n",
    "    \n",
    "\n",
    "\n",
    "X_train=shuffled_X[0:4000]\n",
    "y_train=shuffled_y[0:4000]\n",
    "\n",
    "X_val=shuffled_X[4000:8000]\n",
    "y_val=shuffled_y[4000:8000]\n",
    "\n",
    "X_test=shuffled_X[8000:10000]\n",
    "y_test=shuffled_y[8000:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) (20pts) \n",
    "\n",
    "Now use your gradient descent function to learn the parameters $w$ using 1000 iterations.  You may choose the learning rate and initial parameters $w_0$ for this problem.  Compare both the fixed learning rate and variable learning rate gradient descents side by side (i.e. 2 subplots).\n",
    "\n",
    "For each method, plot the loss of both the training set and the validation set.  Make sure your plots are labeled properly with $x$ and $y$ axis labels, a legend, title, and different colors to distinguish the two curves on each subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with fixed\n",
      "done with variable\n"
     ]
    }
   ],
   "source": [
    "w_init=np.asarray(np.append([0],np.ones(20)))\n",
    "w_init=np.multiply(w_init,0.01)\n",
    "\n",
    "ite=1000\n",
    "\n",
    "def loss_wrt_W(w):\n",
    "    return loss(w,X_train,y_train)\n",
    "\n",
    "def monitored_descent_fixed(loss_fun,x0,lr,iterations,loss_storage):\n",
    "    x=np.asarray(x0).astype('float64')\n",
    "\n",
    "    for i in range(0,iterations):\n",
    "                                                                 \n",
    "        #keeping track of the losses for X_train and X_val\n",
    "        #if (i<=24) or (i % 10==0 and i<=100) or (i % 100 == 0):      \n",
    "        t_loss=loss(x,X_train,y_train)\n",
    "        v_loss=loss(x,X_val,y_val)\n",
    "        loss_storage.append([t_loss,v_loss,int(i)])       \n",
    "        \n",
    "        x=gradient_descent_fixed(loss_fun, x, lr, 1)       \n",
    "    return x\n",
    "\n",
    "def monitored_descent_variable(loss_fun,x0,lr,iterations,loss_storage):\n",
    "    x=np.asarray(x0).astype('float64')\n",
    "\n",
    "    for i in range(0,iterations):\n",
    "\n",
    "        #if (i<=24) or (i % 10==0 and i<=100) or (i % 100 == 0):\n",
    "        t_loss=loss(x,X_train,y_train)\n",
    "        v_loss=loss(x,X_val,y_val)\n",
    "        loss_storage.append([t_loss,v_loss,int(i)]) \n",
    "        \n",
    "        \n",
    "        x=gradient_descent_variable(loss_fun, x, lr, 1)       \n",
    "    return x\n",
    "\n",
    "loss_tracker_fixed=[]\n",
    "w_fixed=monitored_descent_fixed(loss_wrt_W, w_init, 0.000001, ite,loss_tracker_fixed)\n",
    "#np.savetxt(\"weight by fixed lr.txt\",w_fixed)\n",
    "#np.savetxt(\"tracked loss by fixed lr.txt\",loss_tracker_fixed)\n",
    "print(\"done with fixed\") \n",
    "\n",
    "loss_tracker_variable=[]\n",
    "w_variable=monitored_descent_variable(loss_wrt_W, w_init, 0.000005, ite,loss_tracker_variable)\n",
    "#np.savetxt(\"weight by variable lr.txt\",w_variable)\n",
    "#np.savetxt(\"tracked loss by variable lr.txt\",loss_tracker_variable)\n",
    "print(\"done with variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in range(0,25):\n",
    "       #if (i<=24) or (i % 10==0 and i<=100) or (i % 100 == 0):\n",
    "         x.append(i)\n",
    "            \n",
    "\"\"\"I am only going to plot the first 25 iterations since the loss tends to converge wayyyyyy before our model is done training.\"\"\"\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "y1 = np.asarray([loss_tracker_fixed[i][0] for i in range(len(x))])\n",
    "y2 = np.asarray([loss_tracker_fixed[i][1] for i in range(len(x))])\n",
    "ax1.plot(x, y1, 'b',label='Training set')\n",
    "ax1.plot(x,y2,'r',label='Validation set')\n",
    "ax1.set_xlabel(r'iteration')\n",
    "ax1.set_ylabel(r'loss')\n",
    "ax1.set_title(r'fixed lr')\n",
    "plt.legend();\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "z1 = np.asarray([loss_tracker_variable[i][0] for i in range(len(x))])\n",
    "z2 = np.asarray([loss_tracker_variable[i][1] for i in range(len(x))])\n",
    "ax2.plot(x, z1, 'b',label='Training set')\n",
    "ax2.plot(x,z2,'r',label='Validation set')\n",
    "ax2.set_xlabel(r'iteration')\n",
    "ax2.set_ylabel(r'loss')\n",
    "ax2.set_title(r'variable lr')\n",
    "\n",
    "plt.legend();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) (10 pts) Evaluating the model on the test set\n",
    "\n",
    "Finally, we have trained our models and are ready to evaluate them on the test set.  For binary classification one way to check our classifier is to make a confusion matrix of our predictions.\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "\\text{Predict 0, Actual 0} & \\text{Predict 0, Actual 1}\\\\\n",
    "\\text{Predict 1, Actual 0} & \\text{Predict 1, Actual 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The diagonal elements are the number of samples that are correctly classified.\n",
    "\n",
    "Use your trained models to classify samples in the test set according to whether $h(x_i; w) \\ge 0.5$ or not and print the confusion matrices.  Also print the accuracy rate which is just the percentage of correctly classified examples for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With fixed learning rate, C:  [761, 205, 217, 817]  accuracy rate=  78.9 %\n",
      "With variable learning rate, C:  [762, 207, 216, 815]  accuracy rate=  78.85 %\n"
     ]
    }
   ],
   "source": [
    "C_fixed=[0,0,0,0]\n",
    "C_variable=[0,0,0,0]\n",
    "p=1\n",
    "q=1\n",
    "for i in range(0,len(X_test)):\n",
    "    \n",
    "    p=logistic_unit(w_fixed,X_test[i])\n",
    "    \n",
    "    if p>=0.5:\n",
    "        if y_test[i]==1:\n",
    "            C_fixed[3]+=1\n",
    "        elif y_test[i]==0:\n",
    "            C_fixed[2]+=1\n",
    "    elif p<0.5:\n",
    "        if y_test[i]==1:\n",
    "            C_fixed[1]+=1\n",
    "        elif y_test[i]==0:\n",
    "            C_fixed[0]+=1\n",
    "    \n",
    "    q=logistic_unit(w_variable,X_test[i])\n",
    "    if q>=0.5:\n",
    "        if y_test[i]==1:\n",
    "            C_variable[3]+=1\n",
    "        elif y_test[i]==0:\n",
    "            C_variable[2]+=1\n",
    "    elif q<0.5:\n",
    "        if y_test[i]==1:\n",
    "            C_variable[1]+=1\n",
    "        elif y_test[i]==0:\n",
    "            C_variable[0]+=1\n",
    "            \n",
    "acc_fixed=100*(C_fixed[0]+C_fixed[3])/((C_fixed[0]+C_fixed[1]+C_fixed[2]+C_fixed[3]))\n",
    "acc_variable=100*(C_variable[0]+C_variable[3])/((C_variable[0]+C_variable[1]+C_variable[2]+C_variable[3]))\n",
    "print(\"With fixed learning rate, C: \",C_fixed,\" accuracy rate= \",acc_fixed,\"%\")\n",
    "print(\"With variable learning rate, C: \",C_variable,\" accuracy rate= \",acc_variable,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
